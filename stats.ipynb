{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from zipfile import ZipFile\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Print the current working directory\n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ./WUGs/scripts/misc && bash -e usim2data.sh #transform USim to WUG, this script will generate 3 directories: data, source, use_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m spacy download en_core_web_sm #needed for tempowic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ./WUGs/scripts/misc && bash -e evonlp2wug.sh  #transforms tempowic to wug, this line needs to be manually run on the terminal as it needs interaction from the user, it will generate the wugdata directory and delete the source directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m spacy download fi_core_news_sm #needed for cosimlex\n",
    "!python3 -m spacy download hr_core_news_sm #needed for cosimlex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ./WUGs/scripts/misc && bash -e cosimlex2wug.sh #transforms cosimlex to wug, this line needs to be manually run on the terminal as it needs interaction from the user, it will generate directory: wugformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#these lines will create multiple zip file and their unzipped folder in the root of the repo\n",
    "\n",
    "#RuDSI\n",
    "rudsi = 'https://github.com/kategavrishina/RuDSI/tree/main/data' \n",
    "\n",
    "#NorDiaChange\n",
    "nordia1 = 'https://github.com/ltgoslo/nor_dia_change/tree/main/subset1/data'\n",
    "nordia2 = 'https://github.com/ltgoslo/nor_dia_change/tree/main/subset2/data'\n",
    "\n",
    "#RuShiftEval\n",
    "rushifteval1 = 'https://github.com/akutuzov/rushifteval_public/tree/main/durel/rushifteval1/data'\n",
    "rushifteval2 = 'https://github.com/akutuzov/rushifteval_public/tree/main/durel/rushifteval2/data'\n",
    "rushifteval3 = 'https://github.com/akutuzov/rushifteval_public/tree/main/durel/rushifteval3/data'\n",
    "\n",
    "#RuSemShift\n",
    "rusemshift1 = 'https://github.com/juliarodina/RuSemShift/tree/master/rusemshift_1/DWUG/data'\n",
    "rusemshift2 = 'https://github.com/juliarodina/RuSemShift/tree/master/rusemshift_2/DWUG/data'\n",
    "\n",
    "\n",
    "#Discowug 1.1.1 \n",
    "if not os.path.exists(\"discowug.zip\") and not os.path.exists('discowug'):\n",
    "    response = requests.get(\"https://zenodo.org/record/7396225/files/discowug.zip\")\n",
    "    with open('discowug.zip', 'wb') as file:\n",
    "        file.write(response.content)\n",
    "    with ZipFile('discowug.zip', 'r') as discowug:\n",
    "        discowug.extractall()\n",
    "\n",
    "\n",
    "#surel 3.0.0 \n",
    "if not os.path.exists(\"surel.zip\") and not os.path.exists('surel'):\n",
    "    response = requests.get(\"https://zenodo.org/record/5784569/files/surel.zip\")\n",
    "    with open('surel.zip', 'wb') as file:\n",
    "        file.write(response.content)    \n",
    "    with ZipFile('surel.zip', 'r') as surel:\n",
    "        surel.extractall()\n",
    "\n",
    "#durel 3.0.0 \n",
    "if not os.path.exists(\"durel.zip\") and not os.path.exists('durel'):\n",
    "    response = requests.get(\"https://zenodo.org/record/5784453/files/durel.zip\")\n",
    "    with open('durel.zip', 'wb') as file:\n",
    "        file.write(response.content)        \n",
    "    with ZipFile('durel.zip', 'r') as durel:\n",
    "        durel.extractall()\n",
    "\n",
    "#DWUG DE 2.3.0 \n",
    "if not os.path.exists(\"dwug_de.zip\") and not os.path.exists('dwug_de'):\n",
    "    response = requests.get(\"https://zenodo.org/record/7441645/files/dwug_de.zip\")\n",
    "    with open('dwug_de.zip', 'wb') as file:\n",
    "        file.write(response.content)    \n",
    "    with ZipFile('dwug_de.zip', 'r') as dwug_de:\n",
    "        dwug_de.extractall()\n",
    "\n",
    "#RefWUG 1.1.0 \n",
    "if not os.path.exists(\"refwug.zip\") and not os.path.exists('refwug'):\n",
    "    response = requests.get(\"https://zenodo.org/record/5791269/files/refwug.zip\")\n",
    "    with open('refwug.zip', 'wb') as file:\n",
    "        file.write(response.content)    \n",
    "    with ZipFile('refwug.zip', 'r') as refwug:\n",
    "        refwug.extractall()\n",
    "\n",
    "#DWUG EN 2.0.1 \n",
    "if not os.path.exists(\"dwug_en.zip\") and not os.path.exists('dwug_en'):\n",
    "    response = requests.get(\"https://zenodo.org/record/7387261/files/dwug_en.zip\")\n",
    "    with open('dwug_en.zip', 'wb') as file:\n",
    "        file.write(response.content)    \n",
    "    with ZipFile('dwug_en.zip', 'r') as dwug_en:\n",
    "        dwug_en.extractall()\n",
    "\n",
    "\n",
    "#DWUG SV 2.0.1 \n",
    "if not os.path.exists(\"dwug_sv.zip\") and not os.path.exists('dwug_sv'):\n",
    "    response = requests.get(\"https://zenodo.org/record/7389506/files/dwug_sv.zip\")\n",
    "    with open('dwug_sv.zip', 'wb') as file:\n",
    "        file.write(response.content)    \n",
    "    with ZipFile('dwug_sv.zip', 'r') as dwug_sv:\n",
    "        dwug_sv.extractall()\n",
    "\n",
    "\n",
    "#DWUG ES 4.0.0 \n",
    "if not os.path.exists(\"dwug_es.zip\") and not os.path.exists('dwug_es'):\n",
    "    response = requests.get(\"https://zenodo.org/record/6433667/files/dwug_es.zip\")\n",
    "    with open('dwug_es.zip', 'wb') as file:\n",
    "        file.write(response.content)    \n",
    "    with ZipFile('dwug_es.zip', 'r') as dwug_es:\n",
    "        dwug_es.extractall()\n",
    "\n",
    "#DiaWUG 1.1.0 \n",
    "if not os.path.exists(\"diawug.zip\") and not os.path.exists('diawug'):\n",
    "    response = requests.get(\"https://zenodo.org/record/5791193/files/diawug.zip\")\n",
    "    with open('diawug.zip', 'wb') as file:\n",
    "        file.write(response.content)    \n",
    "    with ZipFile('diawug.zip', 'r') as diawug:\n",
    "        diawug.extractall()\n",
    "\n",
    "\n",
    "#DUPS_WUG 2.0.0 \n",
    "if not os.path.exists(\"DUPS-WUG.zip\") and not os.path.exists('DUPS-WUG'):\n",
    "    response = requests.get(\"https://zenodo.org/record/5500223/files/DUPS-WUG.zip\")\n",
    "    with open('DUPS-WUG.zip', 'wb') as file:\n",
    "        file.write(response.content)    \n",
    "    with ZipFile('DUPS-WUG.zip', 'r') as dups:\n",
    "        dups.extractall()\n",
    "\n",
    "# #Discowug 1.1.1 \n",
    "# if not os.path.exists(\"discowug.zip\") and not os.path.exists('discowug'):\n",
    "#     response = requests.get(\"https://zenodo.org/record/7396225/files/discowug.zip\")\n",
    "#     with open('discowug.zip', 'wb') as file:\n",
    "#         file.write(response.content)\n",
    "#     with ZipFile('discowug.zip', 'r') as discowug:\n",
    "#         discowug.extractall()\n",
    "\n",
    "\n",
    "# #surel 3.0.0 \n",
    "# if not os.path.exists(\"surel.zip\") and not os.path.exists('surel'):\n",
    "#     response = requests.get(\"https://zenodo.org/record/5784569/files/surel.zip\")\n",
    "#     with open('surel.zip', 'wb') as file:\n",
    "#         file.write(response.content)    \n",
    "#     with ZipFile('surel.zip', 'r') as surel:\n",
    "#         surel.extractall()\n",
    "\n",
    "# #durel 3.0.0 \n",
    "# if not os.path.exists(\"durel.zip\") and not os.path.exists('durel'):\n",
    "#     response = requests.get(\"https://zenodo.org/record/5784453/files/durel.zip\")\n",
    "#     with open('durel.zip', 'wb') as file:\n",
    "#         file.write(response.content)        \n",
    "#     with ZipFile('durel.zip', 'r') as durel:\n",
    "#         durel.extractall()\n",
    "\n",
    "# #DWUG DE 1.0.0 \n",
    "# if not os.path.exists(\"dwug_de.zip\") and not os.path.exists('dwug_de'):\n",
    "#     response = requests.get(\"https://zenodo.org/record/5543724/files/dwug_de.zip\")\n",
    "#     with open('dwug_de.zip', 'wb') as file:\n",
    "#         file.write(response.content)    \n",
    "#     with ZipFile('dwug_de.zip', 'r') as dwug_de:\n",
    "#         dwug_de.extractall()\n",
    "\n",
    "# #RefWUG 1.1.0 \n",
    "# if not os.path.exists(\"refwug.zip\") and not os.path.exists('refwug'):\n",
    "#     response = requests.get(\"https://zenodo.org/record/5791269/files/refwug.zip\")\n",
    "#     with open('refwug.zip', 'wb') as file:\n",
    "#         file.write(response.content)    \n",
    "#     with ZipFile('refwug.zip', 'r') as refwug:\n",
    "#         refwug.extractall()\n",
    "\n",
    "# #DWUG EN 1.0.0 \n",
    "# if not os.path.exists(\"dwug_en.zip\") and not os.path.exists('dwug_en'):\n",
    "#     response = requests.get(\"https://zenodo.org/record/5544444/files/dwug_en.zip\")\n",
    "#     with open('dwug_en.zip', 'wb') as file:\n",
    "#         file.write(response.content)    \n",
    "#     with ZipFile('dwug_en.zip', 'r') as dwug_en:\n",
    "#         dwug_en.extractall()\n",
    "\n",
    "\n",
    "# #DWUG SV 1.0.0 \n",
    "# if not os.path.exists(\"dwug_sv.zip\") and not os.path.exists('dwug_sv'):\n",
    "#     response = requests.get(\"https://zenodo.org/record/5090648/files/dwug_sv.zip\")\n",
    "#     with open('dwug_sv.zip', 'wb') as file:\n",
    "#         file.write(response.content)    \n",
    "#     with ZipFile('dwug_sv.zip', 'r') as dwug_sv:\n",
    "#         dwug_sv.extractall()\n",
    "\n",
    "\n",
    "# #DWUG ES 4.0.0 \n",
    "# if not os.path.exists(\"dwug_es.zip\") and not os.path.exists('dwug_es'):\n",
    "#     response = requests.get(\"https://zenodo.org/record/6433667/files/dwug_es.zip\")\n",
    "#     with open('dwug_es.zip', 'wb') as file:\n",
    "#         file.write(response.content)    \n",
    "#     with ZipFile('dwug_es.zip', 'r') as dwug_es:\n",
    "#         dwug_es.extractall()\n",
    "\n",
    "# #DiaWUG 1.1.0 \n",
    "# if not os.path.exists(\"diawug.zip\") and not os.path.exists('diawug'):\n",
    "#     response = requests.get(\"https://zenodo.org/record/5791193/files/diawug.zip\")\n",
    "#     with open('diawug.zip', 'wb') as file:\n",
    "#         file.write(response.content)    \n",
    "#     with ZipFile('diawug.zip', 'r') as diawug:\n",
    "#         diawug.extractall()\n",
    "\n",
    "\n",
    "# #DUPS_WUG 2.0.0 \n",
    "# if not os.path.exists(\"DUPS-WUG.zip\") and not os.path.exists('DUPS-WUG'):\n",
    "#     response = requests.get(\"https://zenodo.org/record/5500223/files/DUPS-WUG.zip\")\n",
    "#     with open('DUPS-WUG.zip', 'wb') as file:\n",
    "#         file.write(response.content)    \n",
    "#     with ZipFile('DUPS-WUG.zip', 'r') as dups:\n",
    "#         dups.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./WUGs/scripts/misc/wic2wug.ipynb #transforms WIC dataset to wug, this will create 4 directories in the root repo:dev, test, train, WIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./WUGs/scripts/misc/rawc2wug.py #Raw-C to wug, this will create directory in the repo: raw-c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all data directories extracted from tempowic, cosimlex and wic\n",
    "direc = []\n",
    "i = os.listdir('WUGs/scripts/misc/wugdata')\n",
    "direc.append(i)\n",
    "\n",
    "k = os.listdir('WUGs/scripts/misc/wugformat')\n",
    "direc.append(k)                                   \n",
    "\n",
    "m = os.listdir('WIC')\n",
    "direc.append(m)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = []          #list of directory paths\n",
    "for i in direc[0]:\n",
    "    paths.append('WUGs/scripts/misc/wugdata/'+i+ '/data/')     #tempowic \n",
    "\n",
    "for i in direc[1]:\n",
    "    paths.append('WUGs/scripts/misc/wugformat/'+ i + '/wug_all/data/all') #cosimlex\n",
    "\n",
    "for i in direc[2]:\n",
    "    paths.append('WIC/'+ i +'/') #wic\n",
    "\n",
    "paths.append(\"WUGs/scripts/misc/data/\")  #usim   \n",
    "paths.append(\"raw-c/\") #rawc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = []                       #list of all folders names(lemma wise) in tempowic, cosimlex, wic, usim, rawc\n",
    "for ds in paths:\n",
    "    path = os.listdir(ds)\n",
    "    folders.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final list judgments paths for tempowic, cosimlex and usim\n",
    "path_j = []  \n",
    "pathco = []                                     \n",
    "for i in folders[0]:\n",
    "     pathj = paths[0] + i + \"/judgments.csv\" #tempowic\n",
    "     path_j.append(pathj)\n",
    "for i in folders[2]:\n",
    "     pathj = paths[2] + i + \"/judgments.csv\" #tempowic\n",
    "     path_j.append(pathj)\n",
    "for i in folders[5]:\n",
    "     pathj = paths[5] + i + \"/judgments.csv\"  #tempowic\n",
    "     path_j.append(pathj)\n",
    "\n",
    "     pathj = paths[6] + \"/judgments.csv\" #cosimlex\n",
    "     pathco.append(pathj)\n",
    "\n",
    "     pathj = paths[7]  + \"/judgments.csv\" #cosimlex\n",
    "     pathco.append(pathj)\n",
    "\n",
    "     pathj = paths[8]  + \"/judgments.csv\" #cosimlex\n",
    "     pathco.append(pathj)\n",
    "for i in folders[12]:            #usim\n",
    "     pathj = paths[12] + i + \"/judgments.csv\"\n",
    "     path_j.append(pathj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final list judgments paths and dataframe for wic and rawc\n",
    "path_k = []                                   \n",
    "for i in folders[9]:\n",
    "    pathj = paths[9] + i + \"/judgments.csv\"    #wic\n",
    "    path_k.append(pathj)\n",
    "for i in folders[10]:\n",
    "    pathj = paths[10] + i + \"/judgments.csv\" #wic\n",
    "    path_k.append(pathj)\n",
    "for i in folders[11]:\n",
    "    pathj = paths[11] + i + \"/judgments.csv\"      #wic\n",
    "    path_k.append(pathj)\n",
    "for i in folders[13]:                             #rawc\n",
    "    pathj = paths[13] + i + \"/judgments.csv\"\n",
    "    path_k.append(pathj)\n",
    "#judgements dataframe for rawc and wic datasets\n",
    "raw_df = pd.DataFrame()\n",
    "for i in path_k:\n",
    "   Tmp = pd.read_csv(i, delimiter='\\t', quoting = 3)\n",
    "#    print(i.split('/')[0])\n",
    "   Tmp['dataset'] = i.split('/')[0]\n",
    "   raw_df = pd.concat([raw_df, Tmp])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df['language'] = 'English'\n",
    "raw_df = raw_df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosim_df = pd.DataFrame()             #cosimlex judgments dataframe\n",
    "for i in pathco:\n",
    "   Tmp = pd.read_csv(i, delimiter='\\t', quoting =3)\n",
    "   Tmp['dataset'] = i.split('/')[4]\n",
    "   cosim_df = pd.concat([cosim_df, Tmp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosim_df.loc[cosim_df[\"dataset\"] == \"fi\", \"language\"] = 'Finnish' \n",
    "cosim_df.loc[cosim_df[\"dataset\"] == \"hr\", \"language\"] = 'Croatian'\n",
    "cosim_df.loc[cosim_df[\"dataset\"] == \"en\", \"language\"] = 'English'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosim_df['dataset'] = 'Cosimlex'\n",
    "cosim_df = cosim_df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "judgemt_df = pd.DataFrame()                   #judgments dataframe for tempowic and usim \n",
    "for i in path_j:\n",
    "    Tmp = pd.read_csv(i, delimiter='\\t', quoting =3)\n",
    "    Tmp['dataset'] = i.split('/')[3]\n",
    "    judgemt_df = pd.concat([judgemt_df, Tmp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "judgemt_df.loc[judgemt_df[\"dataset\"] == \"wugdata\", \"dataset\"] = 'TempoWic'\n",
    "judgemt_df.loc[judgemt_df[\"dataset\"] == \"data\", \"dataset\"] = 'USim'\n",
    "     \n",
    "\n",
    "\n",
    "judgemt_df.loc[judgemt_df[\"dataset\"] == \"TempoWic\", \"language\"] = 'English'\n",
    "judgemt_df.loc[judgemt_df[\"dataset\"] == \"USim\", \"language\"] = 'English'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "judgemt_df = judgemt_df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dwugde = \"dwug_de/data\"                          #WUG data directory paths\n",
    "dwugen = \"dwug_en/data\"\n",
    "dwugsv = \"dwug_sv/data\"\n",
    "discowugg = \"discowug/data\"\n",
    "durel = \"durel/data\"\n",
    "surel = \"surel/data\"\n",
    "refwug = \"refwug/data\"\n",
    "dwuges = 'dwug_es/data'\n",
    "diawug = 'diawug/data'\n",
    "dups = 'DUPS-WUG/data'\n",
    "dupswug = ''\n",
    "dwug = [dwugde, dwugen,dwugsv,discowugg, durel, surel, refwug, dwuges, diawug, dups]\n",
    "dirlist = []\n",
    "for dataset in dwug:\n",
    "  dir = os.listdir(dataset)\n",
    "  dirlist.append(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dwug_j = []                                                #dwug data paths\n",
    "for i in dirlist[0]:\n",
    "  dwugde_j = \"dwug_de/data/\" + i + \"/judgments.csv\"\n",
    "  dwug_j.append(dwugde_j)\n",
    "for i in dirlist[1]:\n",
    "  dwugen_j = \"dwug_en/data/\" + i + \"/judgments.csv\"\n",
    "  dwug_j.append(dwugen_j)\n",
    "for i in dirlist[2]:\n",
    "  dwugsv_j = \"dwug_sv/data/\" + i + \"/judgments.csv\"\n",
    "  dwug_j.append(dwugsv_j)\n",
    "for i in dirlist[3]:\n",
    "  discowugg_j = \"discowug/data/\" + i + \"/judgments.csv\"\n",
    "  dwug_j.append(discowugg_j)\n",
    "for i in dirlist[4]:\n",
    "  durel_j = \"durel/data/\" + i + \"/judgments.csv\"\n",
    "  dwug_j.append(durel_j)\n",
    "for i in dirlist[5]:\n",
    "  surel_j = \"surel/data/\" + i + \"/judgments.csv\"\n",
    "  dwug_j.append(surel_j)  \n",
    "for i in dirlist[6]:\n",
    "  refwug_j = \"refwug/data/\" + i + \"/judgments.csv\"\n",
    "  dwug_j.append(refwug_j)\n",
    "for i in dirlist[7]:\n",
    "  dwuges_j = \"dwug_es/data/\" + i + \"/judgments.csv\"\n",
    "  dwug_j.append(dwuges_j)\n",
    "for i in dirlist[8]:\n",
    "  diawug_j = \"diawug/data/\" + i + \"/judgments.csv\"\n",
    "  dwug_j.append(diawug_j)\n",
    "for i in dirlist[9]:\n",
    "  dups_j = \"DUPS-WUG/data/\" + i + \"/judgments.csv\"\n",
    "  dwug_j.append(dups_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "judgemnt_df = pd.DataFrame()            #dwug data judgments df\n",
    "for i in dwug_j:\n",
    "   Tmp = pd.read_csv(i, delimiter='\\t', quoting = 3)\n",
    "   Tmp['dataset'] = i.split('/')[0]\n",
    "   judgemnt_df = pd.concat([judgemnt_df, Tmp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_u = []                      #uses paths for tempowic and usim\n",
    "for i in folders[0]:\n",
    "    pathj = paths[0] + i + \"/uses.csv\"    #tempowic\n",
    "    path_u.append(pathj)\n",
    "for i in folders[2]:\n",
    "    pathj = paths[2] + i + \"/uses.csv\"     #tempowic\n",
    "    path_u.append(pathj)\n",
    "for i in folders[4]:\n",
    "    pathj = paths[4] + i + \"/uses.csv\"     #tempowic\n",
    "    path_u.append(pathj)\n",
    "for i in folders[12]:\n",
    "     pathj = paths[12] + i + \"/uses.csv\"   #usim\n",
    "     path_u.append(pathj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_cou = []                               #for cosimlex uses paths\n",
    "for i in folders[6]:\n",
    "    pathj = paths[6] + \"/uses.csv\"\n",
    "    path_cou.append(pathj)\n",
    "for i in folders[7]:\n",
    "    pathj = paths[7] + \"/uses.csv\"\n",
    "    path_cou.append(pathj)\n",
    "for i in folders[8]:\n",
    "    pathj = paths[8] +  \"/uses.csv\"\n",
    "    path_cou.append(pathj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosim_uses_df = pd.DataFrame()            #cosimlex uses df\n",
    "for i in path_cou:\n",
    "   Tmp = pd.read_csv(i, delimiter='\\t', quoting = 3)\n",
    "   Tmp['dataset'] = i.split('/')[4]\n",
    "   cosim_uses_df = pd.concat([cosim_uses_df, Tmp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosim_uses_df.loc[cosim_uses_df[\"dataset\"] == \"fi\", \"language\"] = 'Finnish' \n",
    "cosim_uses_df.loc[cosim_uses_df[\"dataset\"] == \"hr\", \"language\"] = 'Croatian'\n",
    "cosim_uses_df.loc[cosim_uses_df[\"dataset\"] == \"en\", \"language\"] = 'English'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosim_uses_df['dataset'] = 'Cosimlex'\n",
    "cosim_uses_df = cosim_uses_df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_k = []                            #wic and rawc uses df\n",
    "for i in folders[9]:\n",
    "    pathj = paths[9] + i + \"/uses.csv\" #wic\n",
    "    path_k.append(pathj)\n",
    "for i in folders[10]:\n",
    "    pathj = paths[10] + i + \"/uses.csv\" #wic\n",
    "    path_k.append(pathj)\n",
    "for i in folders[11]:\n",
    "    pathj = paths[11] + i + \"/uses.csv\" #wic\n",
    "    path_k.append(pathj)\n",
    "for i in folders[13]:\n",
    "    pathj = paths[13] + i + \"/uses.csv\" #rawc\n",
    "    path_k.append(pathj)\n",
    "raw_uses_df = pd.DataFrame()\n",
    "for i in path_k:\n",
    "   Tmp = pd.read_csv(i, delimiter='\\t', quoting = 3)\n",
    "#    print(i.split('/')[0])\n",
    "   Tmp['dataset'] = i.split('/')[0]\n",
    "   raw_uses_df = pd.concat([raw_uses_df, Tmp]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_uses_df['language'] = 'English'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_df = pd.DataFrame()    #uses df temp for tempowic and usim\n",
    "for i in path_u:\n",
    "   Tmp = pd.read_csv(i, delimiter='\\t', quoting =3)\n",
    "   Tmp['dataset'] = i.split('/')[3]\n",
    "   use_df = pd.concat([use_df, Tmp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_df.loc[use_df[\"dataset\"] == \"wugdata\", \"dataset\"] = 'TempoWic'\n",
    "use_df.loc[use_df[\"dataset\"] == \"data\", \"dataset\"] = 'USim'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_df['language'] = 'English'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dwug_u = []                                           #dwug data uses paths\n",
    "for i in dirlist[0]:\n",
    "  dwugde_u = \"dwug_de/data/\" + i + \"/uses.csv\"\n",
    "  dwug_u.append(dwugde_u)\n",
    "for i in dirlist[1]:\n",
    "  dwugen_u = \"dwug_en/data/\" + i + \"/uses.csv\"\n",
    "  dwug_u.append(dwugen_u)\n",
    "for i in dirlist[2]:\n",
    "  dwugsv_u = \"dwug_sv/data/\" + i + \"/uses.csv\"\n",
    "  dwug_u.append(dwugsv_u)\n",
    "for i in dirlist[3]:\n",
    "  discowugg_u = \"discowug/data/\" + i + \"/uses.csv\"\n",
    "  dwug_u.append(discowugg_u)\n",
    "for i in dirlist[4]:\n",
    "  durel_u = \"durel/data/\" + i + \"/uses.csv\"\n",
    "  dwug_u.append(durel_u)\n",
    "for i in dirlist[5]:\n",
    "  surel_u = \"surel/data/\" + i + \"/uses.csv\"\n",
    "  dwug_u.append(surel_u)  \n",
    "for i in dirlist[6]:\n",
    "  refwug_u = \"refwug/data/\" + i + \"/uses.csv\"\n",
    "  dwug_u.append(refwug_u)\n",
    "for i in dirlist[7]:\n",
    "  dwuges_u = \"dwug_es/data/\" + i + \"/uses.csv\"\n",
    "  dwug_u.append(dwuges_u)\n",
    "for i in dirlist[8]:\n",
    "  diawug_u = \"diawug/data/\" + i + \"/uses.csv\"\n",
    "  dwug_u.append(diawug_u)\n",
    "\n",
    "for i in dirlist[9]:\n",
    "  dups_u = \"DUPS-WUG/data/\" + i + \"/uses.csv\"\n",
    "  dwug_u.append(dups_u)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "judgemnt_df.loc[judgemnt_df[\"dataset\"] == \"dwug_de\", \"language\"] = 'German'\n",
    "judgemnt_df.loc[judgemnt_df[\"dataset\"] == \"dwug_en\", \"language\"] = 'English'\n",
    "judgemnt_df.loc[judgemnt_df[\"dataset\"] == \"DUPS-WUG\", \"language\"] = 'English'\n",
    "judgemnt_df.loc[judgemnt_df[\"dataset\"] == \"dwug_es\", \"language\"] = 'Spanish'\n",
    "#judgemnt_df.loc[judgemnt_df[\"name\"] == \"dwug_la\", \"language\"] = 'latin'\n",
    "judgemnt_df.loc[judgemnt_df[\"dataset\"] == \"dwug_sv\", \"language\"] = 'Swedish'\n",
    "judgemnt_df.loc[judgemnt_df[\"dataset\"] == \"durel\", \"language\"] = 'German'\n",
    "judgemnt_df.loc[judgemnt_df[\"dataset\"] == \"surel\", \"language\"] = 'German'\n",
    "judgemnt_df.loc[judgemnt_df[\"dataset\"] == \"discowug\", \"language\"] = 'German'\n",
    "judgemnt_df.loc[judgemnt_df[\"dataset\"] == \"refwug\", \"language\"] = 'German'\n",
    "judgemnt_df.loc[judgemnt_df[\"dataset\"] == \"diawug\", \"language\"] = 'Spanish'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final judgments df (without russian and norwegian datasets)\n",
    "judgment_df = pd.DataFrame()\n",
    "judgment_df = pd.concat([judgment_df, judgemt_df], axis = 0)\n",
    "judgment_df = pd.concat([judgment_df, judgemnt_df], axis = 0)    \n",
    "judgment_df = pd.concat([judgment_df, raw_df], axis = 0)\n",
    "judgment_df = pd.concat([judgment_df, cosim_df], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "judgment_df = judgment_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usee_df = pd.DataFrame()            #uses dwug df\n",
    "for i in dwug_u:\n",
    "    Tmp = pd.read_csv(i, delimiter='\\t', quoting = 3)\n",
    "    Tmp['dataset'] = i.split('/')[0]\n",
    "    usee_df = pd.concat([usee_df, Tmp])\n",
    "    usee_df = pd.concat([usee_df, pd.read_csv(i, delimiter='\\t', quoting = 3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usee_df.loc[usee_df[\"dataset\"] == \"dwug_de\", \"language\"] = 'German'\n",
    "usee_df.loc[usee_df[\"dataset\"] == \"dwug_en\", \"language\"] = 'English'\n",
    "usee_df.loc[usee_df[\"dataset\"] == \"DUPS-WUG\", \"language\"] = 'English'\n",
    "usee_df.loc[usee_df[\"dataset\"] == \"dwug_es\", \"language\"] = 'Spanish'\n",
    "#usee_df.loc[usee_df[\"dataset\"] == \"dwug_la\", \"language\"] = 'latin'\n",
    "usee_df.loc[usee_df[\"dataset\"] == \"dwug_sv\", \"language\"] = 'Swedish'\n",
    "usee_df.loc[usee_df[\"dataset\"] == \"durel\", \"language\"] = 'German'\n",
    "usee_df.loc[usee_df[\"dataset\"] == \"surel\", \"language\"] = 'German'\n",
    "usee_df.loc[usee_df[\"dataset\"] == \"discowug\", \"language\"] = 'German'\n",
    "usee_df.loc[usee_df[\"dataset\"] == \"refwug\", \"language\"] = 'German'\n",
    "usee_df.loc[usee_df[\"dataset\"] == \"diawug\", \"language\"] = 'Spanish'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combining uses df\n",
    "uses_full_df = pd.concat([usee_df, use_df], axis = 0)\n",
    "uses1_df = pd.concat([uses_full_df, raw_uses_df], axis = 0)\n",
    "uses_df_full = pd.concat([uses1_df, cosim_uses_df], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_fix = [\"nor_dia_change_1\", \"nor_dia_change_2\", \"rusemshift_1\", \"rusemshift_2\", \"rushifteval1\", \"rushifteval2\", \"rushifteval3\", \"RuDSI\"]\n",
    "\n",
    "df_use_fix = pd.DataFrame()\n",
    "df_jud_fix = pd.DataFrame()\n",
    "for i in range(len(dataset_fix)):\n",
    "    parent_folder = \"dataset/\" + dataset_fix[i] + '/data'\n",
    "    for lemma in os.listdir(parent_folder):\n",
    "        csv_path_jud = os.path.join(parent_folder, lemma, \"judgments.csv\")\n",
    "        csv_path_use = os.path.join(parent_folder, lemma, \"uses.csv\")\n",
    "        df_jud = pd.read_csv(csv_path_jud, delimiter='\\t', quoting =csv.QUOTE_MINIMAL)\n",
    "        df_use = pd.read_csv(csv_path_use, delimiter='\\t', quoting =csv.QUOTE_MINIMAL)\n",
    "        if dataset_fix[i].startswith(\"nor_dia_change\"):\n",
    "            df_use[\"language\"] = \"Norwegian\"\n",
    "            df_use[\"dataset\"] = \"NorDiaChange\"\n",
    "            df_jud[\"language\"] = \"Norwegian\"\n",
    "            df_jud[\"dataset\"] = \"NorDiaChange\"\n",
    "        else:\n",
    "            df_use[\"language\"] = \"Russian\"\n",
    "            if dataset_fix[i].startswith(\"rusemshift\"):\n",
    "                df_use[\"dataset\"] = \"rusemshift\"\n",
    "                df_jud[\"dataset\"] = \"rusemshift\"\n",
    "            elif dataset_fix[i].startswith(\"rushifteval\"):\n",
    "                df_use[\"dataset\"] = \"rushifteval\"\n",
    "                df_jud[\"dataset\"] = \"rushifteval\"\n",
    "            elif dataset_fix[i].startswith(\"RuDSI\"):\n",
    "                df_use[\"dataset\"] = \"RuDSI\"\n",
    "                df_jud[\"dataset\"] = \"RuDSI\"\n",
    "\n",
    "        df_jud_fix = pd.concat([df_jud_fix,df_jud], ignore_index=True)\n",
    "        df_use_fix = pd.concat([df_use_fix,df_use], ignore_index=True)\n",
    "print(df_jud_fix['dataset'].value_counts())\n",
    "print(df_use_fix['dataset'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final judgments dataframe full format\n",
    "judgments_full = pd.concat([judgment_df, df_jud_fix], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final uses dataframe full format\n",
    "uses_full = pd.concat([uses_df_full, df_use_fix], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resetting the index of uses and judgments dataframes because they have repeated indices\n",
    "judgments_full = judgments_full.reset_index(drop= True)\n",
    "uses_full = uses_full.reset_index(drop= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final judgments in wug format\n",
    "judgments_wug = judgments_full[[\"identifier1\", \"identifier2\", \"annotator\", \"judgment\", \"comment\", \"lemma\", \"dataset\", \"language\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final uses dataframe in wug format\n",
    "uses_wug= uses_full[['lemma', 'pos', 'date', 'grouping', 'identifier', 'description', 'context', 'indexes_target_token', 'indexes_target_sentence', 'dataset', 'language']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "judgments_wug.to_csv('judgements_all.csv', sep=\"\\t\", index=False, quoting=csv.QUOTE_NONE)\n",
    "uses_wug.to_csv('uses_all.csv', sep=\"\\t\", index=False, quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import csv\n",
    "\n",
    "df_judgments = pd.read_csv('judgements_all.csv', sep=\"\\t\", quoting =csv.QUOTE_NONE)\n",
    "df_uses = pd.read_csv('uses_all.csv', delimiter='\\t', quoting =3)\n",
    "print(len(df_judgments))\n",
    "print(len(df_uses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_judgments.keys())\n",
    "\n",
    "dataset_list = df_judgments[\"dataset\"].unique().tolist()\n",
    "\n",
    "for dataset_name in dataset_list:\n",
    "    unique_labels_of_dataset = df_judgments[df_judgments['dataset'] == dataset_name][\"judgment\"].unique().tolist()\n",
    "    if np.nan in unique_labels_of_dataset:\n",
    "        print(dataset_name + \" has nan value\")\n",
    "    print(dataset_name + str(unique_labels_of_dataset))\n",
    "\n",
    "df_aggregated = df_judgments[df_judgments['dataset'].isin(['WIC', 'Cosimlex', \"TempoWic\"])]\n",
    "\n",
    "df_judgments = df_judgments[~df_judgments['dataset'].isin(['WIC', 'Cosimlex', \"TempoWic\"])]\n",
    "\n",
    "key_for_vote = 'judgment'\n",
    "\n",
    "# df_judgements_with_vote_five = df_judgments[df_judgments[key_for_vote] == 5]\n",
    "# print(df_judgements_with_vote_five[\"dataset\"].unique())\n",
    "\n",
    "# df_usim = df_judgments[df_judgments['dataset'] == \"USim\"]\n",
    "# df_judgments = df_judgments[df_judgments['dataset'] != \"USim\"]\n",
    "# print(df_judgments[df_judgments['dataset'] == \"USim\"][\"judgment\"].unique())\n",
    "\n",
    "\n",
    "# df_aggregated[key_for_vote].replace({'T': 1, 'F': 0}, inplace=True)\n",
    "\n",
    "row_index_of_null_vote = df_judgments.loc[df_judgments[key_for_vote].isnull()].index.tolist()\n",
    "print(\"number of rows with judgment of nan: \" + str(len(row_index_of_null_vote)))\n",
    "\n",
    "# csv_row_index_null_vote = [index + 2 for index in df_judgements.loc[df_judgements[key_for_vote].isnull()].index]\n",
    "# print(df_judgements.iloc[csv_row_index_null_vote[0:5]])\n",
    "# print(f\"The null value in column 'judgment' is located on csv row {csv_row_index_null_vote}.\")\n",
    "\n",
    "complement =  ~df_judgments.index.isin(row_index_of_null_vote)\n",
    "print(len(df_judgments))\n",
    "\n",
    "df_judgments[\"judgment\"][complement] = df_judgments[\"judgment\"][complement].astype(float)\n",
    "\n",
    "df_judgments[key_for_vote].replace({0.0: np.nan}, inplace=True)\n",
    "\n",
    "print(df_judgments[\"dataset\"].unique())\n",
    "\n",
    "print(df_judgments[\"judgment\"].unique())\n",
    "# print(df_usim[\"judgment\"].unique())\n",
    "\n",
    "print(df_aggregated[\"judgment\"].unique())\n",
    "\n",
    "\n",
    "dataset_list = df_judgments[\"dataset\"].unique().tolist()\n",
    "for dataset_name in dataset_list:\n",
    "    unique_labels_of_dataset = df_judgments[df_judgments['dataset'] == dataset_name][\"judgment\"].unique().tolist()\n",
    "    if np.nan in unique_labels_of_dataset:\n",
    "        print(dataset_name + \" has nan value\")\n",
    "\n",
    "print(df_aggregated.head())\n",
    "print(df_judgments.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nonaggregated_instances_with_multiple_annotations(df_judgments):\n",
    "    key_identifier_one = 'identifier1'\n",
    "    key_identifier_two = 'identifier2'\n",
    "    key_dataset = 'dataset'\n",
    "    key_judgment = \"judgment\"\n",
    "    dict_of_nonaggregated_instances = dict()\n",
    "    for index, row in df_judgments.iterrows():\n",
    "        composite_key = (row[key_dataset], row[key_identifier_one], row[key_identifier_two])\n",
    "        if composite_key not in dict_of_nonaggregated_instances:\n",
    "            dict_of_nonaggregated_instances[composite_key] = [tuple(list(composite_key) + [index, row[key_judgment]])]\n",
    "        else:\n",
    "            dict_of_nonaggregated_instances[composite_key] = dict_of_nonaggregated_instances[composite_key] + [tuple(list(composite_key) + [index, row[key_judgment]])]\n",
    "\n",
    "    dict_of_nonaggregated_instances_with_multiple_annotations = dict()\n",
    "    row_count = 0\n",
    "    for key, value in dict_of_nonaggregated_instances.items():\n",
    "        if len(value) != 1:\n",
    "            dict_of_nonaggregated_instances_with_multiple_annotations[key] = value\n",
    "            row_count = row_count + len(value)\n",
    "        else:\n",
    "            row_count = row_count + 1\n",
    "    print(\"row_count: \" + str(row_count))\n",
    "    return dict_of_nonaggregated_instances_with_multiple_annotations\n",
    "\n",
    "def get_rows_of_nonaggregated_instances_with_multiple_annotations(df_judgments):\n",
    "    rows_of_nonaggregated_instances_with_multiple_annotations = list()\n",
    "    key_identifier_one = 'identifier1'\n",
    "    key_identifier_two = 'identifier2'\n",
    "    key_dataset = 'dataset'\n",
    "    key_judgment = \"judgment\"\n",
    "    dict_of_nonaggregated_instances = dict()\n",
    "    for index, row in df_judgments.iterrows():\n",
    "        composite_key = (row[key_dataset], row[key_identifier_one], row[key_identifier_two])\n",
    "        if composite_key not in dict_of_nonaggregated_instances:\n",
    "            dict_of_nonaggregated_instances[composite_key] = [tuple(list(composite_key) + [index, row[key_judgment]])]\n",
    "        else:\n",
    "            dict_of_nonaggregated_instances[composite_key] = dict_of_nonaggregated_instances[composite_key] + [tuple(list(composite_key) + [index, row[key_judgment]])]\n",
    "\n",
    "    row_count = 0\n",
    "    for key, value in dict_of_nonaggregated_instances.items():\n",
    "        if len(value) != 1:\n",
    "            for tuple_annotation in value:\n",
    "                rows_of_nonaggregated_instances_with_multiple_annotations.append(tuple_annotation[3])\n",
    "            row_count = row_count + len(value)\n",
    "        else:\n",
    "            row_count = row_count + 1\n",
    "    print(\"row_count: \" + str(row_count))\n",
    "    return rows_of_nonaggregated_instances_with_multiple_annotations\n",
    "\n",
    "# rows_of_nonaggregated_instances_with_multiple_annotations = get_rows_of_nonaggregated_instances_with_multiple_annotations(df_judgments)\n",
    "# print(len(rows_of_nonaggregated_instances_with_multiple_annotations))\n",
    "\n",
    "# df_judgments_removing_instances_of_with_multiple_annotations = df_judgments.drop(rows_of_nonaggregated_instances_with_multiple_annotations)\n",
    "\n",
    "# print(len(get_rows_of_nonaggregated_instances_with_multiple_annotations(df_judgments_removing_instances_of_with_multiple_annotations)))\n",
    "\n",
    "def get_votes_for_nonaggregated_instances_with_multiple_annotations(df_judgments):\n",
    "    dict_of_nonaggregated_instances_with_multiple_annotations = get_nonaggregated_instances_with_multiple_annotations(df_judgments)\n",
    "    for key, value in dict_of_nonaggregated_instances_with_multiple_annotations.items():\n",
    "        votes_list = list()\n",
    "        for tuple_annotation in value:\n",
    "            votes_list.append(tuple_annotation[-1])\n",
    "        votes_list.sort()\n",
    "        dict_of_nonaggregated_instances_with_multiple_annotations[key] = votes_list\n",
    "    return dict_of_nonaggregated_instances_with_multiple_annotations\n",
    "\n",
    "# print(get_nonaggregated_instances_with_multiple_annotations(df_judgments.head(2000)))\n",
    "# print(get_votes_for_nonaggregated_instances_with_multiple_annotations(df_judgments.head(2000)))\n",
    "\n",
    "\n",
    "def aggregation_median_rounding(dict_of_nonaggregated_instances_with_multiple_annotations: dict):\n",
    "    for key, value in dict_of_nonaggregated_instances_with_multiple_annotations.items():\n",
    "        array = np.array(value)\n",
    "        median = np.nanmedian(array)\n",
    "        if not np.isnan(median):\n",
    "            dict_of_nonaggregated_instances_with_multiple_annotations[key] = float(round(median))\n",
    "        else:\n",
    "            dict_of_nonaggregated_instances_with_multiple_annotations[key] = np.nan\n",
    "    return dict_of_nonaggregated_instances_with_multiple_annotations\n",
    "\n",
    "# print(aggregation_median_rounding(get_votes_for_nonaggregated_instances_with_multiple_annotations(df_judgments.head(2000))))\n",
    "\n",
    "def aggregation_median_exclusion(dict_of_nonaggregated_instances_with_multiple_annotations: dict):\n",
    "    for key, value in dict_of_nonaggregated_instances_with_multiple_annotations.items():\n",
    "        array = np.array(value)\n",
    "        median = np.nanmedian(array)\n",
    "        if median.is_integer():\n",
    "            dict_of_nonaggregated_instances_with_multiple_annotations[key] = median\n",
    "        else:   \n",
    "            dict_of_nonaggregated_instances_with_multiple_annotations[key] = np.nan\n",
    "    return dict_of_nonaggregated_instances_with_multiple_annotations\n",
    "\n",
    "# print(aggregation_median_exclusion(get_votes_for_nonaggregated_instances_with_multiple_annotations(df_judgments.head(2000))))\n",
    "\n",
    "def cleaning_remove_nan(dict_of_nonaggregated_instances_with_multiple_annotations: dict):\n",
    "    for key, value in dict_of_nonaggregated_instances_with_multiple_annotations.items():\n",
    "        if (np.nan in value):\n",
    "            dict_of_nonaggregated_instances_with_multiple_annotations[key] = [np.nan for _ in range(len(value))]\n",
    "    return dict_of_nonaggregated_instances_with_multiple_annotations\n",
    "\n",
    "def cleaning_remove_any_disagreements(dict_of_nonaggregated_instances_with_multiple_annotations: dict):\n",
    "    dict_of_nonaggregated_instances_with_multiple_annotations = cleaning_remove_nan(dict_of_nonaggregated_instances_with_multiple_annotations)\n",
    "    for key, value in dict_of_nonaggregated_instances_with_multiple_annotations.items():\n",
    "        unique_labels_list = list(set(value))\n",
    "        if len(unique_labels_list) > 1:\n",
    "            dict_of_nonaggregated_instances_with_multiple_annotations[key] = [np.nan for _ in range(len(value))]\n",
    "    return dict_of_nonaggregated_instances_with_multiple_annotations\n",
    "    \n",
    "def cleaning_allow_disagreements_in_one_point(dict_of_nonaggregated_instances_with_multiple_annotations: dict):\n",
    "    dict_of_nonaggregated_instances_with_multiple_annotations = cleaning_remove_nan(dict_of_nonaggregated_instances_with_multiple_annotations)\n",
    "    for key, value in dict_of_nonaggregated_instances_with_multiple_annotations.items():\n",
    "        unique_labels_list = list(set(value))\n",
    "        if len(unique_labels_list) > 2:\n",
    "            dict_of_nonaggregated_instances_with_multiple_annotations[key] = [np.nan for _ in range(len(value))]\n",
    "        if (len(unique_labels_list) == 2):\n",
    "            if abs(unique_labels_list[0] - unique_labels_list[1]) > 1:\n",
    "                dict_of_nonaggregated_instances_with_multiple_annotations[key] = [np.nan for _ in range(len(value))]\n",
    "    return dict_of_nonaggregated_instances_with_multiple_annotations\n",
    "\n",
    "def cleaning_remove_instance_with_one_valid_vote(dict_of_nonaggregated_instances_with_multiple_annotations: dict):\n",
    "    # this condition seems to be unnecessary because cleaning_remove_nan already deal with this situation\n",
    "    pass\n",
    "\n",
    "# print(cleaning_remove_any_disagreements(get_votes_for_nonaggregated_instances_with_multiple_annotations(df_judgments.head(2000))))\n",
    "# print(cleaning_allow_disagreements_in_one_point(get_votes_for_nonaggregated_instances_with_multiple_annotations(df_judgments.head(2000))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregation_median_rounding_cleaning_condition_one(dict_of_nonaggregated_instances_with_multiple_annotations: dict):\n",
    "    dict_of_nonaggregated_instances_with_multiple_annotations = cleaning_remove_any_disagreements(dict_of_nonaggregated_instances_with_multiple_annotations)\n",
    "    return aggregation_median_rounding(dict_of_nonaggregated_instances_with_multiple_annotations)\n",
    "\n",
    "def aggregation_median_exclusion_cleaning_condition_one(dict_of_nonaggregated_instances_with_multiple_annotations: dict):\n",
    "    dict_of_nonaggregated_instances_with_multiple_annotations = cleaning_remove_any_disagreements(dict_of_nonaggregated_instances_with_multiple_annotations)\n",
    "    return aggregation_median_exclusion(dict_of_nonaggregated_instances_with_multiple_annotations)\n",
    "\n",
    "def aggregation_median_rounding_cleaning_condition_two(dict_of_nonaggregated_instances_with_multiple_annotations: dict):\n",
    "    dict_of_nonaggregated_instances_with_multiple_annotations = cleaning_allow_disagreements_in_one_point(dict_of_nonaggregated_instances_with_multiple_annotations)\n",
    "    return aggregation_median_rounding(dict_of_nonaggregated_instances_with_multiple_annotations)\n",
    "\n",
    "def aggregation_median_exclusion_cleaning_condition_two(dict_of_nonaggregated_instances_with_multiple_annotations: dict):\n",
    "    dict_of_nonaggregated_instances_with_multiple_annotations = cleaning_allow_disagreements_in_one_point(dict_of_nonaggregated_instances_with_multiple_annotations)\n",
    "    return aggregation_median_exclusion(dict_of_nonaggregated_instances_with_multiple_annotations)\n",
    "\n",
    "# import copy\n",
    "# dict_of_nonaggregated_instances_with_multiple_annotations = get_votes_for_nonaggregated_instances_with_multiple_annotations(df_judgments.head(2000))\n",
    "# dict_aggregation_median_rounding = aggregation_median_rounding(copy.copy((dict_of_nonaggregated_instances_with_multiple_annotations)))\n",
    "# dict_aggregation_median_exclusion = aggregation_median_exclusion(copy.copy(dict_of_nonaggregated_instances_with_multiple_annotations))\n",
    "\n",
    "# new_dict_of_nonaggregated_instances_with_multiple_annotations = dict()\n",
    "# for key, value in dict_of_nonaggregated_instances_with_multiple_annotations.items():\n",
    "#     new_dict_of_nonaggregated_instances_with_multiple_annotations[key] = {\"median_rounding\": dict_aggregation_median_rounding[key], \"median_exclusion\": dict_aggregation_median_exclusion[key]}\n",
    "\n",
    "# print(new_dict_of_nonaggregated_instances_with_multiple_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "['identifier1', 'identifier2', 'annotator', 'judgment', 'comment',\n",
    "       'lemma', 'dataset', 'language']\n",
    "string_identifier_one = \"identifier1\"\n",
    "string_identifier_two = 'identifier2'\n",
    "string_annotator = \"annotator\"\n",
    "string_judgment = \"judgment\"\n",
    "string_comment = \"comment\"\n",
    "string_lemma = \"lemma\"\n",
    "string_dataset = \"dataset\"\n",
    "string_language = \"language\"\n",
    "string_rounding = \"rounding\"\n",
    "string_exclusion = \"exclusion\"\n",
    "df_judgments[string_rounding] = \"not applicable\"\n",
    "df_judgments[string_exclusion] = \"not applicable\"\n",
    "rows_to_drop = get_rows_of_nonaggregated_instances_with_multiple_annotations(df_judgments)\n",
    "\n",
    "dict_of_nonaggregated_instances_with_multiple_annotations = get_votes_for_nonaggregated_instances_with_multiple_annotations(df_judgments)\n",
    "dict_aggregation_median_rounding = aggregation_median_rounding(copy.copy((dict_of_nonaggregated_instances_with_multiple_annotations)))\n",
    "dict_aggregation_median_exclusion = aggregation_median_exclusion(copy.copy(dict_of_nonaggregated_instances_with_multiple_annotations))\n",
    "\n",
    "new_dict_of_nonaggregated_instances_with_multiple_annotations = dict()\n",
    "for key, value in dict_of_nonaggregated_instances_with_multiple_annotations.items():\n",
    "    new_dict_of_nonaggregated_instances_with_multiple_annotations[key] = {string_rounding: dict_aggregation_median_rounding[key], string_exclusion: dict_aggregation_median_exclusion[key]}\n",
    "\n",
    "# for index, row in df_judgments.itered_insrows():\n",
    "#     key_identifier_one = 'identifier1'\n",
    "#     key_identifier_two = 'identifier2'\n",
    "#     key_dataset = 'dataset'\n",
    "#     composite_key = (row[key_dataset], row[key_identifier_one], row[key_identifier_two])\n",
    "#     if (composite_key in new_dict_of_nonaggregated_instances_with_multiple_annotations):\n",
    "#         df_judgments.at[index, string_rounding] = new_dict_of_nonaggregated_instances_with_multiple_annotations[composite_key][string_rounding]\n",
    "#         df_judgments.at[index, string_exclusion] = new_dict_of_nonaggregated_instances_with_multiple_annotations[composite_key][string_exclusion]\n",
    "\n",
    "aggregated_rows = list()\n",
    "for key, value in new_dict_of_nonaggregated_instances_with_multiple_annotations.items():\n",
    "    row = {string_identifier_one: key[1], string_identifier_two: key[2], string_annotator: \"gold\", string_judgment:\"not applicable\", string_comment:\"gone through aggregation\", string_lemma: \"not applicable\", string_dataset: key[0], string_language: \"not applicable\", string_rounding:new_dict_of_nonaggregated_instances_with_multiple_annotations[key][string_rounding], string_exclusion:new_dict_of_nonaggregated_instances_with_multiple_annotations[key][string_exclusion]}\n",
    "    aggregated_rows.append(row)\n",
    "\n",
    "df_to_be_appended = pd.DataFrame(aggregated_rows)\n",
    "print(df_to_be_appended.sample(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_judgments.drop(rows_to_drop, inplace=True)\n",
    "merged_df = pd.concat([df_judgments, df_to_be_appended])\n",
    "print(merged_df.sample(10))\n",
    "\n",
    "print(get_nonaggregated_instances_with_multiple_annotations(merged_df))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stats-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
